
# üß† Tech Challenge ‚Äì An√°lise Inteligente de V√≠deo com IA (Vis√£o Computacional)

Este projeto foi desenvolvido como parte do **Tech Challenge da FIAP (Fase 4 ‚Äì Intelig√™ncia Artificial)** e tem como objetivo a cria√ß√£o de uma **aplica√ß√£o de an√°lise autom√°tica de v√≠deo**, utilizando t√©cnicas avan√ßadas de **Vis√£o Computacional, Deep Learning e IA Generativa**.

A aplica√ß√£o √© capaz de:
- Identificar pessoas em v√≠deo
- Analisar express√µes emocionais faciais
- Detectar e categorizar atividades humanas
- Detectar comportamentos an√¥malos
- Gerar automaticamente um resumo estruturado do conte√∫do analisado

---

## üéØ Objetivo do Projeto

Aplicar na pr√°tica os conhecimentos adquiridos ao longo da fase, integrando m√∫ltiplos modelos de IA para realizar uma **an√°lise sem√¢ntica e comportamental de v√≠deos**, simulando cen√°rios reais como reuni√µes de trabalho, uso de computadores, intera√ß√µes sociais, atividades expressivas (dan√ßa, gestos) e situa√ß√µes fora do padr√£o.

---

## üìÅ Estrutura do Projeto

```
Tech-Challenge-4-IA-FIAP/
‚îú‚îÄ‚îÄ assets/
‚îÇ   ‚îî‚îÄ‚îÄ input_video.mp4
‚îú‚îÄ‚îÄ outputs/
‚îÇ   ‚îú‚îÄ‚îÄ annotated_video.mp4
‚îÇ   ‚îú‚îÄ‚îÄ report.txt
‚îÇ   ‚îî‚îÄ‚îÄ events.json
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îú‚îÄ‚îÄ pipeline/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ person_detector.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clip_zeroshot.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ action_recog.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ emotion_deepface.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anomaly.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ summarizer.py
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îî‚îÄ‚îÄ video_utils.py
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öôÔ∏è Como Executar

### 0) Pr√©-requisitos
- **Python 3.11** (recomendado)  
- Windows PowerShell (ou terminal VS Code)  
- V√≠deo em `assets/input_video.mp4`

### 1) Criar ambiente virtual
```powershell
py -3.11 -m venv .venv
.venv\Scripts\Activate.ps1
python -m pip install -U pip
```

### 2) Instalar depend√™ncias do projeto
```powershell
pip install -r requirements.txt
```

---

## üöÄ Rodar em GPU NVIDIA CUDA (Recomendado)

GPU antiga:
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

GPU nova por exemplo RTX 5070 arquitetura Blackwell e tem CUDA capability sm_120
```bash
pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128
```

### üîé Verifica√ß√£o - validar GPU (PyTorch)
```bash
python -c "import torch; print(torch.__version__); print('cuda:', torch.cuda.is_available()); print('cap:', torch.cuda.get_device_capability(0)); print(torch.cuda.get_device_name(0))"
```

```powershell
python -m src.main `
  --video "assets/input_video.mp4" `
  --out "outputs" `
  --device cuda `
  --frame-skip 2 `
  --clip-len 16
```


> üí° **Observa√ß√£o importante (RTX 5070 / sm_120):**  
> Algumas GPUs muito novas podem exigir uma vers√£o do PyTorch com suporte atualizado. Se aparecer erro de compatibilidade, instale uma build mais recente (nightly) conforme instru√ß√£o acima, ou use CPU temporariamente.

---

## üñ•Ô∏è Rodar em CPU (Fallback ‚Äì mais lento)

Se voc√™ n√£o tiver GPU NVIDIA (ou n√£o estiver configurada), rode em CPU:

```bash
pip install torch torchvision torchaudio
```

```powershell
python -m src.main `
  --video "assets/input_video.mp4" `
  --out "outputs" `
  --device cpu `
  --frame-skip 2 `
  --clip-len 16
```

‚ö†Ô∏è **Aviso:** em CPU o processamento demora mais (pode levar de **20 a 40+ minutos**, dependendo do v√≠deo e das configura√ß√µes).

---

## üîß Par√¢metros √∫teis (para qualidade x performance)

- `--frame-skip 2`  
  Analisa 1 frame a cada 2 (menos custo, mais r√°pido).  
  Para mais qualidade, use `--frame-skip 1`.

- `--clip-len 16`  
  N√∫mero de frames por ‚Äúclip‚Äù para a√ß√µes.  
  Aumentar ajuda a√ß√µes cont√≠nuas, mas custa mais.

---

## üìä Sa√≠das Geradas

Ap√≥s rodar, voc√™ ter√°:

- `outputs/annotated_video.mp4`  
  V√≠deo com caixas (pessoa/face), IDs, atividade e emo√ß√£o.

- `outputs/report.txt`  
  Relat√≥rio autom√°tico com:
  - total de frames analisados
  - n√∫mero de anomalias detectadas
  - ranking de atividades
  - emo√ß√µes por pessoa
  - atividades por pessoa
  - amostras de anomalias

- `outputs/events.json`  
  Log detalhado por frame (√∫til para auditoria/debug).

---

## üß† T√©cnicas Utilizadas

### 1) Detec√ß√£o & Tracking de Pessoas
- **YOLOv8 (Ultralytics)** para detectar pessoas
- **ByteTrack** para manter um ID consistente ao longo do v√≠deo

### 2) Emo√ß√µes Faciais (por pessoa)
- **DeepFace** para inferir emo√ß√µes (happy, sad, angry, fear, surprise, neutral, etc.)
- Associa√ß√£o emo√ß√£o ‚Üî pessoa via proximidade box pessoa / face

### 3) Atividades (por pessoa e no geral)
Abordagem h√≠brida (mais robusta que ‚Äúum modelo s√≥‚Äù):
- **CLIP Zero-Shot (OpenCLIP)** com prompts em ingl√™s (mais ‚Äúhumanos‚Äù) e **labels final em portugu√™s**
- **Action Recognition (R3D-18 / Kinetics400)** como *fallback* quando o CLIP n√£o est√° confiante
- Heur√≠sticas simples para atividades ‚Äúcontextuais‚Äù, ex:
  - **reuni√£o / conversa** (pessoas pr√≥ximas, postura, baixa movimenta√ß√£o)
  - **usando computador / digitando** (pessoa sentada + m√£os perto da regi√£o de teclado/mesa + objetos pr√≥ximos)

### 4) Anomalias
- Anomalia = movimento fora do padr√£o geral do v√≠deo (gestos bruscos, mudan√ßas abruptas etc.)
- Implementa√ß√£o: **z-score** do deslocamento/varia√ß√£o de posi√ß√£o ao longo do tempo

### 5) Suaviza√ß√£o temporal (anti ‚Äúalucina√ß√£o‚Äù)
- Vota√ß√£o/janela temporal para reduzir troca de labels frame a frame
- ‚Äúcooldown‚Äù m√≠nimo antes de mudar a atividade dominante

---


## üìö Bibliotecas Principais

- `torch` - Backend de deep learning utilizado para executar modelos de IA em CPU ou GPU (CUDA), incluindo Action Recognition e CLIP
- `ultralytics` - Implementa√ß√£o do YOLOv8 para detec√ß√£o e tracking de pessoas em v√≠deos
- `open-clip-torch` - Implementa√ß√£o do CLIP Zero-Shot, utilizada para classifica√ß√£o sem√¢ntica de atividades em linguagem natural
- `deepface` - Biblioteca para an√°lise de express√µes emocionais faciais, baseada em modelos pr√©-treinados
- `opencv-python` - ‚Äì Processamento de v√≠deo, leitura de frames, escrita de v√≠deo anotado e opera√ß√µes de imagem
- `numpy` - ‚Äì Opera√ß√µes num√©ricas, manipula√ß√£o de arrays e c√°lculos estat√≠sticos (ex: detec√ß√£o de anomalias)
- `tqdm` ‚Äì Exibi√ß√£o de barras de progresso durante o processamento do v√≠deo
- `mediapipe` - Extra√ß√£o de landmarks corporais e faciais, auxiliando na an√°lise de postura e movimentos
- `protobuf` - Serializa√ß√£o de dados utilizada internamente pelo MediaPipe e TensorFlow
- `keras` - API de alto n√≠vel para constru√ß√£o e execu√ß√£o de modelos neurais utilizados pelo DeepFace
- `gast` - Depend√™ncia do ecossistema TensorFlow para an√°lise e transforma√ß√£o de grafos computacionais
- `tensorboard` - Ferramenta de visualiza√ß√£o e monitoramento utilizada pelo TensorFlow
- `pillow` - Manipula√ß√£o e convers√£o de imagens, suporte auxiliar ao OpenCV e CLIP

---

## üë®‚Äçüíª Autor

**Phellype Guilherme Pereira da Silva**  
**RM:** 361625  
**Projeto:** Fase 4 - P√≥s Tech FIAP - Intelig√™ncia Artificial  
**Institui√ß√£o:** [FIAP ‚Äì Faculdade de Inform√°tica e Administra√ß√£o Paulista](https://www.fiap.com.br/)
